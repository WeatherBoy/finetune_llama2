# General Notes
## Fine-tuning Llama2
### Practical approach
#### Meta's own documentation
This is Meta's own documentation and it is a really good ressource for using Llama 2, [LINK](https://ai.meta.com/llama/get-started/).

Furtehremore, there is this specific section on ressources which looks really good! [LINK](https://ai.meta.com/llama/get-started/#:~:text=issues%20using%20Asana.-,Resources,-Github).

#### Medium
A very good Medium article: [How to Fine-Tune Llama2 for Python Coding on Consumer Hardware
](https://towardsdatascience.com/how-to-fine-tune-llama2-for-python-coding-on-consumer-hardware-46942fa3cf92).


### Theoretical approach
#### LoRA
Look at and read this paper:
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685

#### QLoRA
Look at and read this paper:
Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314.


## MLOps
